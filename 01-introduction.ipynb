{
 "metadata": {
  "name": "",
  "signature": "sha256:982cdc7004e406678e72245e5e91a09d8b6bbc3528cc78313fd1f05f0345dbe3"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "<img src=\"images/continuum_analytics_logo.png\" \n",
      "                                alt=\"Continuum Logo\",\n",
      "                                align=\"right\",\n",
      "                                width=\"30%\">,\n",
      "\n",
      "Introduction to Blaze\n",
      "=====================\n",
      "\n",
      "In this 45 minute tutorial we'll learn how to use Blaze to discover, migrate, and query data living in other databases.  Generally this tutorial will have the following format\n",
      "\n",
      "1. `into` - Move data to database\n",
      "2. `blaze` - Query data in database\n",
      "3. `remote` - What if data and database is on an HDFS-backed cluster?\n",
      "\n",
      "\n",
      "Goal: Accessible, Interactive, Analytic Queries\n",
      "-----------------------------------------------\n",
      "\n",
      "NumPy and Pandas provide accessible, interactive, analytic queries; this is valuable."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "df = pd.read_csv('iris.csv')\n",
      "df.head()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "zipimport: can not open file /Users/aterrel/workspace/apps/anaconda/miniconda2/lib/python2.7/site-packages/setuptools-12.0.5-py2.7.egg\n"
       ]
      },
      {
       "ename": "IOError",
       "evalue": "zipimport: can not open file /Users/aterrel/workspace/apps/anaconda/miniconda2/lib/python2.7/site-packages/setuptools-12.0.5-py2.7.egg",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mIOError\u001b[0m                                   Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-1-cf8c25e33423>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'iris.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/aterrel/workspace/apps/anaconda/miniconda2/lib/python2.7/site-packages/pandas/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mhashtable\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtslib\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pragma: no cover\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;32m/Users/aterrel/workspace/docs/pydata-strata-2014-sj/pandas/tslib.pyx\u001b[0m in \u001b[0;36minit pandas.tslib (pandas/tslib.c:79900)\u001b[0;34m()\u001b[0m\n",
        "\u001b[0;32m/Users/aterrel/workspace/apps/anaconda/miniconda2/lib/python2.7/site-packages/pytz/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0mpkg_resources\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mresource_stream\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mresource_stream\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
        "\u001b[0;31mIOError\u001b[0m: zipimport: can not open file /Users/aterrel/workspace/apps/anaconda/miniconda2/lib/python2.7/site-packages/setuptools-12.0.5-py2.7.egg"
       ]
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.groupby(df.species).petal_length.mean()  # Average petal length per species"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "NameError",
       "evalue": "name 'df' is not defined",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
        "\u001b[0;32m<ipython-input-2-87c7110bdc23>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mspecies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpetal_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# Average petal length per species\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "If your data fits on your computer then this is probably the way to go, and you can stop reading right now.  \n",
      "\n",
      "From now on, we're going to assume one of the following:\n",
      "\n",
      "1.  You have an inconvenient amount of data\n",
      "2.  That data should live someplace other than your computer"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Databases and Python\n",
      "--------------------\n",
      "\n",
      "When in-memory arrays/dataframes cease to be an option, we turn to databases.  These live outside of the Python process and so might be less convenient.  The open source Python ecosystem includes libraries to interact with these databases and with foreign data in general.  \n",
      "\n",
      "Examples:\n",
      "\n",
      "* SQL - [`sqlalchemy`](http://sqlalchemy.org) \n",
      "    * Hive/Cassandra - [`pyhive`](https://github.com/dropbox/PyHive)\n",
      "    * Impala  - [`impyla`](https://github.com/cloudera/impyla)\n",
      "    * RedShift - [`redshift-sqlalchemy`](https://pypi.python.org/pypi/redshift-sqlalchemy)\n",
      "    * ...\n",
      "* MongoDB - [`pymongo`](http://api.mongodb.org/python/current/)\n",
      "* HBase - [`happybase`](http://happybase.readthedocs.org/en/latest/)\n",
      "* Spark - [`pyspark`](http://spark.apache.org/docs/latest/api/python/)\n",
      "* SSH - [`paramiko`](http://www.paramiko.org/)\n",
      "* HDFS - [`pywebhdfs`](https://pypi.python.org/pypi/pywebhdfs)\n",
      "* Amazon S3 - [`boto`](https://boto.readthedocs.org/en/latest/)\n",
      "\n",
      "Today we're going to use some of these indirectly with `into` and Blaze.  We'll try to point out these libraries as we automate them so that, if you'd like, you can use them independently."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Blaze and `into`\n",
      "----------------\n",
      "\n",
      "The Blaze and `into` projects give a consistent interface over many of the libraries above.  They strive to trivialize common tasks.\n",
      "\n",
      "* `into` moves data from place to place and from format to format\n",
      "* `blaze` queries data in databases\n",
      "\n",
      "We're going to start with `into`, learning how to migrate data between formats, between computers, and into databases.  We'll then use Blaze to perform analytic queries on that data."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Teaser\n",
      "------\n",
      "\n",
      "We'll eventually do things like this\n",
      "\n",
      "```Python\n",
      ">>> from into import into\n",
      ">>> into('hive://hostname/default::iris', 'iris.csv')  # Move local data onto HDFS and register with Hive\n",
      "\n",
      ">>> from blaze import Data, by\n",
      ">>> db = Data('hive://hostname/default')\n",
      ">>> by(db.species, avg=db.petal_length.mean())\n",
      "...\n",
      "```\n",
      "\n",
      "Not a Magic Bullet\n",
      "------------------\n",
      "\n",
      "Blaze and `into` make easy things trivial but are not a replacement for intimate knowledge of your database or for your problem.  These projects are intended to enable non-expert users to do every-day tasks."
     ]
    }
   ],
   "metadata": {}
  }
 ]
}